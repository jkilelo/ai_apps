# Dockerfile for Spark 3.3.2 with Cloudera CDH 7.1.9
FROM centos:7

# Fix CentOS 7 EOL repository issues
RUN sed -i 's/mirrorlist/#mirrorlist/g' /etc/yum.repos.d/CentOS-* && \
    sed -i 's|#baseurl=http://mirror.centos.org|baseurl=http://vault.centos.org|g' /etc/yum.repos.d/CentOS-*

# Set environment variables
ENV SPARK_VERSION=3.3.2
ENV CDH_VERSION=7.1.9
ENV HADOOP_VERSION=3.1.1
ENV JAVA_HOME=/usr/lib/jvm/java-8-openjdk
ENV SPARK_HOME=/opt/spark
ENV HADOOP_HOME=/opt/cloudera/parcels/CDH
ENV PYTHON_VERSION=3.9.20
ENV PYSPARK_PYTHON=/usr/bin/python3.9
ENV PYSPARK_DRIVER_PYTHON=/usr/bin/python3.9
ENV PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin:$HADOOP_HOME/bin:$JAVA_HOME/bin:/usr/local/bin

# Install system dependencies
RUN yum update -y && \
    yum install -y \
        java-1.8.0-openjdk \
        java-1.8.0-openjdk-devel \
        wget \
        curl \
        which \
        tar \
        gzip \
        rsync \
        openssh-server \
        openssh-clients \
        sudo \
        net-tools \
        vim \
        git \
        gcc \
        gcc-c++ \
        make \
        openssl-devel \
        bzip2-devel \
        libffi-devel \
        zlib-devel \
        xz-devel \
        sqlite-devel && \
    yum clean all

# Install Python 3.9.20 from source
RUN cd /tmp && \
    wget https://www.python.org/ftp/python/3.9.20/Python-3.9.20.tgz && \
    tar -xzf Python-3.9.20.tgz && \
    cd Python-3.9.20 && \
    ./configure --enable-optimizations --enable-shared && \
    make -j$(nproc) && \
    make altinstall && \
    cd / && \
    rm -rf /tmp/Python-3.9.20* && \
    ln -sf /usr/local/bin/python3.9 /usr/bin/python3 && \
    ln -sf /usr/local/bin/python3.9 /usr/bin/python && \
    ln -sf /usr/local/bin/pip3.9 /usr/bin/pip3 && \
    ln -sf /usr/local/bin/pip3.9 /usr/bin/pip && \
    echo '/usr/local/lib' >> /etc/ld.so.conf && \
    ldconfig

# Configure Cloudera repository
RUN rpm --import https://archive.cloudera.com/cdh7/7.1.9.0/redhat7/yum/RPM-GPG-KEY-cloudera && \
    wget https://archive.cloudera.com/cdh7/7.1.9.0/redhat7/yum/cloudera-cdh7.repo -O /etc/yum.repos.d/cloudera-cdh7.repo

# Install Cloudera CDH components
RUN yum install -y \
        cloudera-manager-daemons \
        cloudera-manager-agent \
        cloudera-manager-server \
        hadoop-hdfs \
        hadoop-hdfs-namenode \
        hadoop-hdfs-datanode \
        hadoop-yarn \
        hadoop-yarn-resourcemanager \
        hadoop-yarn-nodemanager \
        hadoop-mapreduce \
        hadoop-client \
        hive \
        hive-metastore \
        hive-server2 \
        zookeeper \
        zookeeper-server && \
    yum clean all

# Download and install Spark 3.3.2
RUN wget -q https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop3.tgz && \
    tar -xzf spark-${SPARK_VERSION}-bin-hadoop3.tgz -C /opt && \
    mv /opt/spark-${SPARK_VERSION}-bin-hadoop3 ${SPARK_HOME} && \
    rm spark-${SPARK_VERSION}-bin-hadoop3.tgz

# Install Python dependencies
RUN pip install --upgrade pip && \
    pip install \
        pyspark==${SPARK_VERSION} \
        numpy \
        pandas \
        pyarrow \
        faker \
        jupyter \
        jupyterlab \
        matplotlib \
        seaborn \
        scikit-learn

# Configure Hadoop
RUN mkdir -p /var/lib/hadoop-hdfs && \
    mkdir -p /var/log/hadoop-hdfs && \
    mkdir -p /etc/hadoop/conf && \
    mkdir -p /data/hdfs/namenode && \
    mkdir -p /data/hdfs/datanode

# Copy Hadoop configuration files
COPY hadoop-conf/* /etc/hadoop/conf/

# Configure Spark
RUN mkdir -p ${SPARK_HOME}/conf
COPY spark-conf/* ${SPARK_HOME}/conf/

# Configure Hive
RUN mkdir -p /user/hive/warehouse && \
    mkdir -p /var/lib/hive && \
    mkdir -p /etc/hive/conf

COPY hive-conf/* /etc/hive/conf/

# Initialize HDFS
RUN mkdir -p /opt/init-scripts
COPY scripts/init-hdfs.sh /opt/init-scripts/
RUN chmod +x /opt/init-scripts/init-hdfs.sh

# Create startup script
COPY scripts/start-services.sh /opt/
RUN chmod +x /opt/start-services.sh

# Expose ports
# HDFS ports
EXPOSE 9870 9864 9866 9867 9868 9869
# YARN ports
EXPOSE 8088 8042
# Spark ports
EXPOSE 4040 7077 8080 8081 18080
# Hive ports
EXPOSE 10000 10002 9083
# Jupyter
EXPOSE 8888

# Set working directory
WORKDIR /workspace

# Create workspace directory
RUN mkdir -p /workspace/data && \
    mkdir -p /workspace/notebooks && \
    mkdir -p /workspace/logs

# Entry point
ENTRYPOINT ["/opt/start-services.sh"]