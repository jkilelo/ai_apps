{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a15ef80",
   "metadata": {},
   "source": [
    "# PySpark 3.4 Demo Notebook\n",
    "\n",
    "This notebook demonstrates PySpark 3.4 functionality in a Podman container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbab83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, avg, max as spark_max\n",
    "\n",
    "print(f\"PySpark version: {pyspark.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c0caa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark Notebook Demo\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Spark UI: http://localhost:4040\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21e7345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data\n",
    "data = [\n",
    "    (\"Alice\", 25, \"Engineering\", 75000),\n",
    "    (\"Bob\", 30, \"Sales\", 65000),\n",
    "    (\"Charlie\", 35, \"Engineering\", 85000),\n",
    "    (\"Diana\", 28, \"Marketing\", 60000),\n",
    "    (\"Eve\", 32, \"Engineering\", 90000),\n",
    "    (\"Frank\", 29, \"Sales\", 70000)\n",
    "]\n",
    "\n",
    "columns = [\"name\", \"age\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7b56d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic aggregations\n",
    "print(\"Department Statistics:\")\n",
    "df.groupBy(\"department\") \\\n",
    "  .agg(count(\"*\").alias(\"count\"),\n",
    "       avg(\"salary\").alias(\"avg_salary\"),\n",
    "       spark_max(\"salary\").alias(\"max_salary\")) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19531db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter high earners\n",
    "print(\"High earners (>70k):\")\n",
    "df.filter(col(\"salary\") > 70000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96c3554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL operations\n",
    "df.createOrReplaceTempView(\"employees\")\n",
    "\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT department, \n",
    "           COUNT(*) as employee_count,\n",
    "           ROUND(AVG(salary), 2) as avg_salary\n",
    "    FROM employees \n",
    "    GROUP BY department \n",
    "    ORDER BY avg_salary DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"SQL Query Results:\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9991ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization with matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Convert to Pandas for plotting\n",
    "pandas_df = df.toPandas()\n",
    "\n",
    "# Plot salary by department\n",
    "plt.figure(figsize=(10, 6))\n",
    "pandas_df.groupby('department')['salary'].mean().plot(kind='bar')\n",
    "plt.title('Average Salary by Department')\n",
    "plt.ylabel('Salary')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bcfa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session (run when done)\n",
    "# spark.stop()\n",
    "print(\"Spark session is still active. Run spark.stop() when done.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
