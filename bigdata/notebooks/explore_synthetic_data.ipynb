{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Synthetic Big Data\n",
    "\n",
    "This notebook demonstrates how to explore and analyze the synthetic data generated for the retail analytics database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Create Spark session with Hive support\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Explore Synthetic Data\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"/user/hive/warehouse\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set display options\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.maxNumRows\", 10)\n",
    "\n",
    "print(\"Spark Session initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Database Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all tables in retail_analytics database\n",
    "spark.sql(\"USE retail_analytics\")\n",
    "tables_df = spark.sql(\"SHOW TABLES\")\n",
    "tables_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get record counts for all tables\n",
    "table_names = [row.tableName for row in tables_df.collect()]\n",
    "\n",
    "record_counts = []\n",
    "for table in table_names:\n",
    "    count = spark.sql(f\"SELECT COUNT(*) as count FROM {table}\").collect()[0].count\n",
    "    record_counts.append({'table': table, 'record_count': count})\n",
    "\n",
    "counts_df = pd.DataFrame(record_counts)\n",
    "counts_df = counts_df.sort_values('record_count', ascending=False)\n",
    "\n",
    "# Plot record counts\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(counts_df['table'], counts_df['record_count'])\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title('Record Counts by Table')\n",
    "plt.ylabel('Number of Records')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(counts_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Customer Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load customers data\n",
    "customers_df = spark.sql(\"SELECT * FROM customers\")\n",
    "customers_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer segmentation analysis\n",
    "segment_analysis = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        customer_segment,\n",
    "        COUNT(*) as customer_count,\n",
    "        AVG(credit_score) as avg_credit_score,\n",
    "        AVG(annual_income) as avg_annual_income,\n",
    "        SUM(CASE WHEN marketing_opt_in = true THEN 1 ELSE 0 END) as marketing_opted_in\n",
    "    FROM customers\n",
    "    GROUP BY customer_segment\n",
    "    ORDER BY customer_count DESC\n",
    "\"\"\")\n",
    "\n",
    "segment_analysis.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer distribution by state\n",
    "state_dist = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        state,\n",
    "        COUNT(*) as customer_count\n",
    "    FROM customers\n",
    "    GROUP BY state\n",
    "    ORDER BY customer_count DESC\n",
    "    LIMIT 20\n",
    "\"\"\")\n",
    "\n",
    "state_dist_pd = state_dist.toPandas()\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.bar(state_dist_pd['state'], state_dist_pd['customer_count'])\n",
    "plt.title('Top 20 States by Customer Count')\n",
    "plt.xlabel('State')\n",
    "plt.ylabel('Number of Customers')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sales Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monthly sales trend\n",
    "monthly_sales = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        DATE_FORMAT(transaction_date, 'yyyy-MM') as month,\n",
    "        COUNT(DISTINCT transaction_id) as num_transactions,\n",
    "        SUM(total_amount) as total_revenue,\n",
    "        AVG(total_amount) as avg_transaction_value\n",
    "    FROM transactions\n",
    "    WHERE transaction_status = 'Completed'\n",
    "    GROUP BY DATE_FORMAT(transaction_date, 'yyyy-MM')\n",
    "    ORDER BY month\n",
    "\"\"\")\n",
    "\n",
    "monthly_sales_pd = monthly_sales.toPandas()\n",
    "\n",
    "# Plot revenue trend\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))\n",
    "\n",
    "# Revenue trend\n",
    "ax1.plot(monthly_sales_pd['month'], monthly_sales_pd['total_revenue'], marker='o')\n",
    "ax1.set_title('Monthly Revenue Trend')\n",
    "ax1.set_xlabel('Month')\n",
    "ax1.set_ylabel('Total Revenue ($)')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Transaction count trend\n",
    "ax2.bar(monthly_sales_pd['month'], monthly_sales_pd['num_transactions'], alpha=0.7)\n",
    "ax2.set_title('Monthly Transaction Count')\n",
    "ax2.set_xlabel('Month')\n",
    "ax2.set_ylabel('Number of Transactions')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top selling products\n",
    "top_products = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        p.product_name,\n",
    "        p.category,\n",
    "        COUNT(DISTINCT t.transaction_id) as num_transactions,\n",
    "        SUM(t.quantity) as total_quantity_sold,\n",
    "        SUM(t.total_amount) as total_revenue\n",
    "    FROM transactions t\n",
    "    JOIN products p ON t.product_id = p.product_id\n",
    "    WHERE t.transaction_status = 'Completed'\n",
    "    GROUP BY p.product_name, p.category\n",
    "    ORDER BY total_revenue DESC\n",
    "    LIMIT 20\n",
    "\"\"\")\n",
    "\n",
    "top_products.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sales by category\n",
    "category_sales = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        p.category,\n",
    "        COUNT(DISTINCT t.transaction_id) as num_transactions,\n",
    "        SUM(t.total_amount) as total_revenue,\n",
    "        AVG(t.total_amount) as avg_transaction_value\n",
    "    FROM transactions t\n",
    "    JOIN products p ON t.product_id = p.product_id\n",
    "    WHERE t.transaction_status = 'Completed'\n",
    "    GROUP BY p.category\n",
    "    ORDER BY total_revenue DESC\n",
    "\"\"\")\n",
    "\n",
    "category_sales_pd = category_sales.toPandas()\n",
    "\n",
    "# Pie chart of revenue by category\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.pie(category_sales_pd['total_revenue'], \n",
    "        labels=category_sales_pd['category'], \n",
    "        autopct='%1.1f%%',\n",
    "        startangle=90)\n",
    "plt.title('Revenue Distribution by Product Category')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Store Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store performance metrics\n",
    "store_performance = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        s.store_name,\n",
    "        s.store_type,\n",
    "        s.city,\n",
    "        s.state,\n",
    "        COUNT(DISTINCT t.transaction_id) as num_transactions,\n",
    "        COUNT(DISTINCT t.customer_id) as unique_customers,\n",
    "        SUM(t.total_amount) as total_revenue,\n",
    "        AVG(t.total_amount) as avg_transaction_value,\n",
    "        s.annual_revenue_target,\n",
    "        (SUM(t.total_amount) / s.annual_revenue_target * 100) as target_achievement_pct\n",
    "    FROM stores s\n",
    "    LEFT JOIN transactions t ON s.store_id = t.store_id AND t.transaction_status = 'Completed'\n",
    "    WHERE s.is_active = true\n",
    "    GROUP BY s.store_id, s.store_name, s.store_type, s.city, s.state, s.annual_revenue_target\n",
    "    ORDER BY total_revenue DESC\n",
    "    LIMIT 20\n",
    "\"\"\")\n",
    "\n",
    "store_performance.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store type comparison\n",
    "store_type_analysis = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        s.store_type,\n",
    "        COUNT(DISTINCT s.store_id) as num_stores,\n",
    "        AVG(s.employees_count) as avg_employees,\n",
    "        SUM(t.total_amount) as total_revenue,\n",
    "        AVG(t.total_amount) as avg_transaction_value\n",
    "    FROM stores s\n",
    "    LEFT JOIN transactions t ON s.store_id = t.store_id AND t.transaction_status = 'Completed'\n",
    "    GROUP BY s.store_type\n",
    "    ORDER BY total_revenue DESC\n",
    "\"\"\")\n",
    "\n",
    "store_type_analysis.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Customer Behavior Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device usage analysis\n",
    "device_analysis = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        device_type,\n",
    "        COUNT(*) as event_count,\n",
    "        COUNT(DISTINCT session_id) as num_sessions,\n",
    "        COUNT(DISTINCT customer_id) as unique_users,\n",
    "        AVG(time_on_page_seconds) as avg_time_on_page,\n",
    "        SUM(CASE WHEN bounce = true THEN 1 ELSE 0 END) * 100.0 / COUNT(*) as bounce_rate\n",
    "    FROM customer_behavior\n",
    "    GROUP BY device_type\n",
    "    ORDER BY event_count DESC\n",
    "\"\"\")\n",
    "\n",
    "device_analysis.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer journey funnel\n",
    "funnel_analysis = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        event_type,\n",
    "        COUNT(DISTINCT session_id) as num_sessions,\n",
    "        COUNT(DISTINCT customer_id) as unique_customers\n",
    "    FROM customer_behavior\n",
    "    WHERE event_type IN ('page_view', 'product_view', 'add_to_cart', 'checkout_start', 'purchase')\n",
    "    GROUP BY event_type\n",
    "\"\"\")\n",
    "\n",
    "funnel_pd = funnel_analysis.toPandas()\n",
    "\n",
    "# Define funnel order\n",
    "funnel_order = ['page_view', 'product_view', 'add_to_cart', 'checkout_start', 'purchase']\n",
    "funnel_pd['event_type'] = pd.Categorical(funnel_pd['event_type'], categories=funnel_order, ordered=True)\n",
    "funnel_pd = funnel_pd.sort_values('event_type')\n",
    "\n",
    "# Create funnel chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(funnel_pd['event_type'], funnel_pd['num_sessions'], alpha=0.7)\n",
    "plt.title('Customer Journey Funnel')\n",
    "plt.xlabel('Event Type')\n",
    "plt.ylabel('Number of Sessions')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Add conversion rates\n",
    "for i in range(1, len(funnel_pd)):\n",
    "    conv_rate = (funnel_pd.iloc[i]['num_sessions'] / funnel_pd.iloc[i-1]['num_sessions']) * 100\n",
    "    plt.text(i-0.5, funnel_pd.iloc[i]['num_sessions'] + 1000, \n",
    "             f'{conv_rate:.1f}%', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Product Reviews Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rating distribution\n",
    "rating_dist = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        rating,\n",
    "        COUNT(*) as review_count,\n",
    "        AVG(helpful_count) as avg_helpful_count\n",
    "    FROM product_reviews\n",
    "    GROUP BY rating\n",
    "    ORDER BY rating\n",
    "\"\"\")\n",
    "\n",
    "rating_dist_pd = rating_dist.toPandas()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(rating_dist_pd['rating'], rating_dist_pd['review_count'])\n",
    "plt.title('Product Rating Distribution')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Number of Reviews')\n",
    "plt.xticks([1, 2, 3, 4, 5])\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category ratings comparison\n",
    "category_ratings = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        p.category,\n",
    "        COUNT(r.review_id) as num_reviews,\n",
    "        AVG(r.rating) as avg_rating,\n",
    "        STDDEV(r.rating) as rating_stddev,\n",
    "        SUM(CASE WHEN r.rating >= 4 THEN 1 ELSE 0 END) * 100.0 / COUNT(*) as positive_review_pct\n",
    "    FROM product_reviews r\n",
    "    JOIN products p ON r.product_id = p.product_id\n",
    "    GROUP BY p.category\n",
    "    HAVING COUNT(r.review_id) > 100\n",
    "    ORDER BY avg_rating DESC\n",
    "\"\"\")\n",
    "\n",
    "category_ratings.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Marketing Campaign ROI Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Campaign performance analysis\n",
    "campaign_performance = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        campaign_type,\n",
    "        COUNT(*) as num_campaigns,\n",
    "        AVG(budget) as avg_budget,\n",
    "        AVG(actual_spend) as avg_spend,\n",
    "        AVG(roi) as avg_roi,\n",
    "        SUM(revenue_generated) as total_revenue,\n",
    "        AVG(ctr) as avg_ctr,\n",
    "        AVG(conversion_rate) as avg_conversion_rate\n",
    "    FROM marketing_campaigns\n",
    "    WHERE status = 'Completed'\n",
    "    GROUP BY campaign_type\n",
    "    ORDER BY avg_roi DESC\n",
    "\"\"\")\n",
    "\n",
    "campaign_performance.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROI by campaign goal\n",
    "roi_by_goal = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        campaign_goal,\n",
    "        campaign_type,\n",
    "        AVG(roi) as avg_roi,\n",
    "        COUNT(*) as num_campaigns\n",
    "    FROM marketing_campaigns\n",
    "    WHERE status = 'Completed' AND roi IS NOT NULL\n",
    "    GROUP BY campaign_goal, campaign_type\n",
    "    ORDER BY campaign_goal, avg_roi DESC\n",
    "\"\"\")\n",
    "\n",
    "roi_by_goal_pd = roi_by_goal.toPandas()\n",
    "\n",
    "# Create grouped bar chart\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "goals = roi_by_goal_pd['campaign_goal'].unique()\n",
    "x = np.arange(len(goals))\n",
    "width = 0.15\n",
    "\n",
    "for i, ctype in enumerate(roi_by_goal_pd['campaign_type'].unique()):\n",
    "    data = roi_by_goal_pd[roi_by_goal_pd['campaign_type'] == ctype]\n",
    "    values = []\n",
    "    for goal in goals:\n",
    "        goal_data = data[data['campaign_goal'] == goal]\n",
    "        values.append(goal_data['avg_roi'].values[0] if len(goal_data) > 0 else 0)\n",
    "    ax.bar(x + i * width, values, width, label=ctype)\n",
    "\n",
    "ax.set_xlabel('Campaign Goal')\n",
    "ax.set_ylabel('Average ROI (%)')\n",
    "ax.set_title('Marketing Campaign ROI by Goal and Type')\n",
    "ax.set_xticks(x + width * 2)\n",
    "ax.set_xticklabels(goals, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Advanced Analytics - Customer Lifetime Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate customer metrics for CLV\n",
    "customer_metrics = spark.sql(\"\"\"\n",
    "    WITH customer_transactions AS (\n",
    "        SELECT \n",
    "            c.customer_id,\n",
    "            c.customer_segment,\n",
    "            c.registration_date,\n",
    "            COUNT(DISTINCT t.transaction_id) as num_transactions,\n",
    "            SUM(t.total_amount) as total_spent,\n",
    "            AVG(t.total_amount) as avg_transaction_value,\n",
    "            MIN(t.transaction_date) as first_purchase,\n",
    "            MAX(t.transaction_date) as last_purchase,\n",
    "            DATEDIFF(MAX(t.transaction_date), MIN(t.transaction_date)) as customer_lifetime_days\n",
    "        FROM customers c\n",
    "        LEFT JOIN transactions t ON c.customer_id = t.customer_id AND t.transaction_status = 'Completed'\n",
    "        GROUP BY c.customer_id, c.customer_segment, c.registration_date\n",
    "    )\n",
    "    SELECT \n",
    "        customer_segment,\n",
    "        COUNT(*) as num_customers,\n",
    "        AVG(total_spent) as avg_customer_value,\n",
    "        AVG(num_transactions) as avg_transactions_per_customer,\n",
    "        AVG(customer_lifetime_days) as avg_customer_lifetime_days,\n",
    "        SUM(total_spent) / COUNT(*) as revenue_per_customer\n",
    "    FROM customer_transactions\n",
    "    WHERE total_spent > 0\n",
    "    GROUP BY customer_segment\n",
    "    ORDER BY avg_customer_value DESC\n",
    "\"\"\")\n",
    "\n",
    "customer_metrics.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close Spark session\n",
    "spark.stop()\n",
    "print(\"Analysis complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}