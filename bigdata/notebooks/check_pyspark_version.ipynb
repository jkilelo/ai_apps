{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Guide: Checking PySpark Version\n",
    "\n",
    "This notebook demonstrates multiple ways to check PySpark version in different environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Direct Module Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Direct import\n",
    "try:\n",
    "    import pyspark\n",
    "    print(f\"PySpark version: {pyspark.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"PySpark is not installed\")\n",
    "except AttributeError:\n",
    "    print(\"Version attribute not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: SparkSession (most common in production)\n",
    "try:\n",
    "    from pyspark.sql import SparkSession\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"VersionCheck\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    print(f\"PySpark version: {spark.version}\")\n",
    "    print(f\"Spark UI: http://localhost:4040\")\n",
    "    \n",
    "    # Additional Spark info\n",
    "    print(f\"\\nSpark Configuration:\")\n",
    "    print(f\"Master: {spark.sparkContext.master}\")\n",
    "    print(f\"App Name: {spark.sparkContext.appName}\")\n",
    "    print(f\"Application ID: {spark.sparkContext.applicationId}\")\n",
    "    \n",
    "    spark.stop()\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3: SparkContext (legacy approach)\n",
    "try:\n",
    "    from pyspark import SparkContext, SparkConf\n",
    "    conf = SparkConf().setAppName(\"VersionCheck\")\n",
    "    sc = SparkContext.getOrCreate(conf)\n",
    "    \n",
    "    print(f\"PySpark version: {sc.version}\")\n",
    "    print(f\"Python version: {sc.pythonVer}\")\n",
    "    print(f\"Spark home: {sc._jsc.sc().getSparkHome().get() if sc._jsc.sc().getSparkHome().isDefined() else 'Not set'}\")\n",
    "    \n",
    "    sc.stop()\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Shell Commands in Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 4a: Using pip\n",
    "!pip show pyspark | grep -E \"Name:|Version:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 4b: Using pip list\n",
    "!pip list | grep -i pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 4c: Using conda (if available)\n",
    "!conda list pyspark 2>/dev/null | grep -v \"^#\" || echo \"Conda not available or PySpark not installed via conda\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. System Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 5: Check system environment\n",
    "import os\n",
    "import sys\n",
    "\n",
    "print(\"System Information:\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"\\nEnvironment variables:\")\n",
    "print(f\"SPARK_HOME: {os.environ.get('SPARK_HOME', 'Not set')}\")\n",
    "print(f\"PYSPARK_PYTHON: {os.environ.get('PYSPARK_PYTHON', 'Not set')}\")\n",
    "print(f\"PYSPARK_DRIVER_PYTHON: {os.environ.get('PYSPARK_DRIVER_PYTHON', 'Not set')}\")\n",
    "\n",
    "# Check if PySpark is in Python path\n",
    "pyspark_paths = [p for p in sys.path if 'pyspark' in p.lower()]\n",
    "if pyspark_paths:\n",
    "    print(f\"\\nPySpark in Python path:\")\n",
    "    for path in pyspark_paths:\n",
    "        print(f\"  - {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Detailed Package Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 6: Get detailed package info\n",
    "try:\n",
    "    import pkg_resources\n",
    "    pyspark_pkg = pkg_resources.get_distribution('pyspark')\n",
    "    print(f\"Package: {pyspark_pkg.key}\")\n",
    "    print(f\"Version: {pyspark_pkg.version}\")\n",
    "    print(f\"Location: {pyspark_pkg.location}\")\n",
    "    \n",
    "    # Get dependencies\n",
    "    print(\"\\nDependencies:\")\n",
    "    for req in pyspark_pkg.requires():\n",
    "        print(f\"  - {req}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not get package info: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Spark Configuration Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 7: Get Spark configuration details\n",
    "try:\n",
    "    from pyspark.sql import SparkSession\n",
    "    spark = SparkSession.builder.appName(\"ConfigCheck\").getOrCreate()\n",
    "    \n",
    "    print(\"Spark Configuration:\")\n",
    "    conf = spark.sparkContext.getConf()\n",
    "    for item in sorted(conf.getAll(), key=lambda x: x[0]):\n",
    "        if any(keyword in item[0].lower() for keyword in ['version', 'python', 'memory', 'cores']):\n",
    "            print(f\"{item[0]}: {item[1]}\")\n",
    "    \n",
    "    spark.stop()\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Version Compatibility Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 8: Check version compatibility\n",
    "import sys\n",
    "\n",
    "def check_compatibility():\n",
    "    try:\n",
    "        import pyspark\n",
    "        pyspark_version = pyspark.__version__\n",
    "        python_version = f\"{sys.version_info.major}.{sys.version_info.minor}\"\n",
    "        \n",
    "        print(f\"PySpark version: {pyspark_version}\")\n",
    "        print(f\"Python version: {python_version}\")\n",
    "        \n",
    "        # Version compatibility matrix\n",
    "        compatibility = {\n",
    "            \"3.5\": [\"3.10\", \"3.11\"],\n",
    "            \"3.4\": [\"3.8\", \"3.9\", \"3.10\", \"3.11\"],\n",
    "            \"3.3\": [\"3.7\", \"3.8\", \"3.9\", \"3.10\"],\n",
    "            \"3.2\": [\"3.6\", \"3.7\", \"3.8\", \"3.9\"],\n",
    "            \"3.1\": [\"3.6\", \"3.7\", \"3.8\", \"3.9\"],\n",
    "            \"3.0\": [\"3.6\", \"3.7\", \"3.8\"],\n",
    "            \"2.4\": [\"2.7\", \"3.4\", \"3.5\", \"3.6\", \"3.7\"],\n",
    "        }\n",
    "        \n",
    "        pyspark_major = pyspark_version.split('.')[0] + '.' + pyspark_version.split('.')[1]\n",
    "        \n",
    "        if pyspark_major in compatibility:\n",
    "            supported_python = compatibility[pyspark_major]\n",
    "            if python_version in supported_python:\n",
    "                print(f\"✅ Python {python_version} is compatible with PySpark {pyspark_version}\")\n",
    "            else:\n",
    "                print(f\"⚠️  Python {python_version} may not be fully compatible with PySpark {pyspark_version}\")\n",
    "                print(f\"   Recommended Python versions: {', '.join(supported_python)}\")\n",
    "        else:\n",
    "            print(f\"ℹ️  Compatibility information not available for PySpark {pyspark_version}\")\n",
    "            \n",
    "    except ImportError:\n",
    "        print(\"PySpark not installed\")\n",
    "\n",
    "check_compatibility()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Docker Container Commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Docker commands to check PySpark version\n",
    "print(\"Docker commands to check PySpark version:\")\n",
    "print(\"\\n1. Check in running container:\")\n",
    "print('   docker exec <container_name> python -c \"import pyspark; print(pyspark.__version__)\"')\n",
    "print(\"\\n2. Check using spark-submit:\")\n",
    "print(\"   docker exec <container_name> spark-submit --version\")\n",
    "print(\"\\n3. Check using pip:\")\n",
    "print(\"   docker exec <container_name> pip show pyspark\")\n",
    "print(\"\\n4. Interactive check:\")\n",
    "print(\"   docker exec -it <container_name> pyspark\")\n",
    "print(\"   >>> spark.version\")\n",
    "print(\"\\n5. Check in Docker image:\")\n",
    "print('   docker run --rm <image_name> python -c \"import pyspark; print(pyspark.__version__)\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Create Version Check Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a reusable version check function\n",
    "def get_pyspark_info():\n",
    "    \"\"\"Get comprehensive PySpark version and environment information\"\"\"\n",
    "    info = {}\n",
    "    \n",
    "    try:\n",
    "        import pyspark\n",
    "        info['installed'] = True\n",
    "        info['version'] = pyspark.__version__\n",
    "        \n",
    "        # Try to get more details from SparkSession\n",
    "        try:\n",
    "            from pyspark.sql import SparkSession\n",
    "            spark = SparkSession.builder.appName(\"InfoCheck\").getOrCreate()\n",
    "            info['spark_version'] = spark.version\n",
    "            info['spark_home'] = spark.sparkContext._jsc.sc().getSparkHome().get() if spark.sparkContext._jsc.sc().getSparkHome().isDefined() else None\n",
    "            info['master'] = spark.sparkContext.master\n",
    "            spark.stop()\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "    except ImportError:\n",
    "        info['installed'] = False\n",
    "        \n",
    "    # Environment info\n",
    "    import os\n",
    "    info['spark_home_env'] = os.environ.get('SPARK_HOME')\n",
    "    info['pyspark_python'] = os.environ.get('PYSPARK_PYTHON')\n",
    "    \n",
    "    return info\n",
    "\n",
    "# Display the information\n",
    "import json\n",
    "info = get_pyspark_info()\n",
    "print(json.dumps(info, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}