# PySpark 3.3.2 Container with Python 3.9.20
FROM python:3.9.20-slim

# Set environment variables
ENV SPARK_VERSION=3.3.2
ENV HADOOP_VERSION=3
ENV SPARK_HOME=/opt/spark
ENV PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
ENV JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64

# Install system dependencies
RUN apt-get update && apt-get install -y \
    openjdk-17-jre-headless \
    wget \
    curl \
    procps \
    && rm -rf /var/lib/apt/lists/*

# Download and install Spark
RUN cd /opt && \
    wget -q https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz && \
    tar -xzf spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz && \
    mv spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION} spark && \
    rm spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz

# Set PYTHONPATH after Spark installation
ENV PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.9.5-src.zip:$PYTHONPATH

# Create working directory
WORKDIR /workspace

# Copy requirements and install Python dependencies
COPY requirements_pyspark.txt .
RUN pip install --no-cache-dir -r requirements_pyspark.txt

# Copy application files
COPY . .

# Expose ports for Spark UI and application
EXPOSE 4040 8080 8081 7077 8888

# Set default command
CMD ["python", "-c", "import pyspark; print(f'PySpark {pyspark.__version__} is ready!')"]
